![image](https://github.com/user-attachments/assets/160c3b8a-4a2c-40eb-afbe-1d38731d00f1)


# Web Crawler para Recuperación de Información
Este proyecto es un **web crawler desarrollado en Java**, diseñado para recorrer directorios, procesar archivos de texto y construir un índice invertido. Posteriormente, permite buscar palabras específicas y obtener su frecuencia en los archivos analizados.

### Características
- Exploración Iterativa de directorios y archivos de texto.
- Procesamiento de texto para contar la frecuencia de palabras.
- Almacenamiento y carga del índice en un archivo serializado.
- Búsqueda optimizada de términos con soporte para múltiples archivos.
- Implementación de un sistema de conocimiento usando un Thesauro.

---

![image](https://github.com/user-attachments/assets/a40f1efd-1f91-41f1-9a97-c7ae290d830e)

*Estructura de Ficheros en árbol a explorar*

### Modo de Uso
- **Indexación de archivos:** Al ejecutarlo, el programa recorrerá los archivos en el directorio E1 y generará un diccionario con las frecuencias de las palabras.
- **Búsqueda de palabras:** En la segunda fase, puedes introducir palabras clave y el sistema te mostrará su frecuencia en cada archivo.
- **Salida del programa:** Para salir de la búsqueda, simplemente presiona ESC.

### Autores: Víctor Navareño, Sergio Terrazas Lobato, Muhammad Zain Din
